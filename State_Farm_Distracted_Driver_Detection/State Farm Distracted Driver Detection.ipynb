{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90b4822f",
   "metadata": {},
   "source": [
    "# State Farm Distracted Driver Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23c8205",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef9d4e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ultralytics\n",
    "# %pip install split-folders\n",
    "# %pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84310787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from glob import glob\n",
    "from shutil import copyfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, BatchNormalization, Dropout\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from ultralytics import YOLO\n",
    "import splitfolders\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, Image\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f481f2d4",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6be8bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "with open('../datasets/state-farm-distracted-driver-detection/driver_imgs_list.csv') as file:\n",
    "    read_file = csv.reader(file)\n",
    "    read_file = list(read_file)\n",
    "    \n",
    "    for row in read_file[1:]:\n",
    "        key = row[1]\n",
    "        if key in data:\n",
    "            data[key].append(row[2])\n",
    "        else:\n",
    "            data[key] = [row[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc9a7866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['img_44733.jpg',\n",
       " 'img_72999.jpg',\n",
       " 'img_25094.jpg',\n",
       " 'img_69092.jpg',\n",
       " 'img_92629.jpg']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['c0'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c68d42fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_list = list(data.keys())\n",
    "classes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5c3952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = '../datasets/state-farm-distracted-driver-detection/imgs/'\n",
    "\n",
    "train_dir = os.path.join(dataset_folder, 'train/')\n",
    "test_dir = os.path.join(dataset_folder, 'test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3d18d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the training dataset :  22424\n",
      "Number of images in the testing dataset :  79726\n"
     ]
    }
   ],
   "source": [
    "print('Number of images in the training dataset : ', str(len(glob(train_dir+'*/*'))))\n",
    "print('Number of images in the testing dataset : ', str(len(glob(test_dir+'*'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a884b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to separate the training set and the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430f7c96",
   "metadata": {},
   "source": [
    "### Writing helper function for creating directories for training set, validation set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb19ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_directory(path):\n",
    "    for root, dirs, files in os.walk(path, topdown = False):\n",
    "        for name in files:\n",
    "            file_path = os.path.join(root, name)\n",
    "            os.remove(file_path)\n",
    "        for name in dirs:\n",
    "            dir_path = os.path.join(root, name)\n",
    "            os.rmdir(dir_path)\n",
    "    os.rmdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7afddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directories(paths, subfolders):\n",
    "    for path in paths:\n",
    "        if os.path.exists(path):\n",
    "            remove_directory(path)\n",
    "        \n",
    "        for folder in subfolders:\n",
    "            subfolder_path = os.path.join(path, folder)\n",
    "            os.makedirs(subfolder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08784b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['../datasets/state-farm-distracted-driver-detection/cleaned_dataset/train',\n",
    "         '../datasets/state-farm-distracted-driver-detection/cleaned_dataset/val',\n",
    "        '../datasets/state-farm-distracted-driver-detection/cleaned_dataset/test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1368dd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "subfolders = classes_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b054be96",
   "metadata": {},
   "source": [
    "### Creating Train, Val, Test folders along with sub-directories (all Classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "823f4b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_directories(paths, subfolders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6412fc",
   "metadata": {},
   "source": [
    "### Creating the cleaned dataset using the above helper functions we have created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a0f7f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_size = [0.6, 0.2]\n",
    "\n",
    "\n",
    "for clas, images in data.items():\n",
    "    # print(len(images))\n",
    "    train_size = int(split_size[0]*len(images))\n",
    "    # print(\"Train size: \", train_size)\n",
    "    \n",
    "    test_size = int(split_size[1]*len(images))\n",
    "    #print(\"Test size: \", test_size)\n",
    "    \n",
    "    train_images = images[:train_size]\n",
    "    # print(\"Train Images Length\", len(train_images))\n",
    "    \n",
    "    val_images = images[train_size: train_size + test_size]\n",
    "    # print(\"Val Images Length\", len(val_images))\n",
    "    \n",
    "    test_images = images[train_size + test_size:]\n",
    "    # print(\"Test Images Length\", len(test_images))\n",
    "    \n",
    "    \n",
    "    \n",
    "    for image in train_images:\n",
    "        source = os.path.join(train_dir, clas, image)\n",
    "        # print(os.path.exists(source))\n",
    "        dest = os.path.join(paths[0], clas, image)\n",
    "        copyfile(source, dest)\n",
    "    \n",
    "    for image in val_images:\n",
    "        source = os.path.join(train_dir, clas, image)\n",
    "        dest = os.path.join(paths[1], clas, image)\n",
    "        copyfile(source, dest)\n",
    "    \n",
    "    for image in test_images:\n",
    "        source = os.path.join(train_dir, clas, image)\n",
    "        dest = os.path.join(paths[2], clas, image)\n",
    "        copyfile(source, dest)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c12170",
   "metadata": {},
   "source": [
    "### Using a better approach for creating the cleaned dataset using `splitfolders` module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f398e856",
   "metadata": {},
   "source": [
    "### First deleting the cleaned dataset created usinfg the above method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57444964",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_directory('../datasets/state-farm-distracted-driver-detection/cleaned_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492fbc52",
   "metadata": {},
   "source": [
    "### Creating the cleaned dataset now using splitfolder module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0950781d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 22424 files [00:03, 5949.75 files/s]\n"
     ]
    }
   ],
   "source": [
    "images_dir = '../datasets/state-farm-distracted-driver-detection/imgs/train'\n",
    "output_folder = '../datasets/state-farm-distracted-driver-detection/cleaned_dataset' # Note: the function will create val, train, test sub directories by itself\n",
    "split_ratio = (0.6, 0.2, 0.2)\n",
    "\n",
    "\n",
    "\n",
    "splitfolders.ratio(images_dir, output= output_folder, seed = 10, ratio= split_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba802fdb",
   "metadata": {},
   "source": [
    "Done ! Just needed one line of code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f31c070",
   "metadata": {},
   "source": [
    "### From now, we will be using these Directory paths for our training, validation and testing purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1676b2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = '../datasets/state-farm-distracted-driver-detection/cleaned_dataset'\n",
    "train_dir = '../datasets/state-farm-distracted-driver-detection/cleaned_dataset/train'\n",
    "val_dir = '../datasets/state-farm-distracted-driver-detection/cleaned_dataset/val'\n",
    "test_dir = '../datasets/state-farm-distracted-driver-detection/cleaned_dataset/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a7ac97",
   "metadata": {},
   "source": [
    "## Creating Image data generator Function with Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b5b9a52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imagedatageneration(train_dir, val_dir, test_dir, target_size = (256, 256), batch_size = 32):\n",
    "    train_datagen = ImageDataGenerator(rescale = 1.0 / 255,\n",
    "                                       rotation_range = 30,\n",
    "                                       width_shift_range = 0.1,\n",
    "                                       height_shift_range = 0.1,\n",
    "                                       zoom_range = 0.1,\n",
    "                                       shear_range = 0.1,\n",
    "                                       fill_mode = \"nearest\"\n",
    "                                      )\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "                                                            train_dir,\n",
    "                                                            target_size = target_size,\n",
    "                                                            class_mode = 'categorical',\n",
    "                                                            shuffle = True,\n",
    "                                                            batch_size = batch_size\n",
    "                                                        )\n",
    "    \n",
    "    \n",
    "    val_datagen = ImageDataGenerator(rescale = 1.0 / 255,\n",
    "                                     rotation_range = 30,\n",
    "                                     width_shift_range = 0.1,\n",
    "                                     height_shift_range = 0.1,\n",
    "                                     zoom_range = 0.1,\n",
    "                                     shear_range = 0.1,\n",
    "                                     fill_mode = \"nearest\"\n",
    "                                    )\n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "                                                        val_dir,\n",
    "                                                        target_size = target_size,\n",
    "                                                        class_mode = 'categorical',\n",
    "                                                        shuffle = True,\n",
    "                                                        batch_size = batch_size\n",
    "                                                    )\n",
    "    \n",
    "    test_datagen = ImageDataGenerator(rescale = 1.0/255)\n",
    "    \n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "                                                        test_dir,\n",
    "                                                        target_size = target_size,\n",
    "                                                        class_mode = 'categorical',\n",
    "                                                        shuffle = False,\n",
    "                                                        batch_size = 1\n",
    "                                                      )\n",
    "    \n",
    "    return train_generator, val_generator, test_generator\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b379af4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "341d837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor = 'val_acc', patience = 2, min_delta = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e970a87",
   "metadata": {},
   "source": [
    "## First Model -> Dense Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bfe4ee8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13451 images belonging to 10 classes.\n",
      "Found 4481 images belonging to 10 classes.\n",
      "Found 4492 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator, val_generator, test_generator = imagedatageneration(train_dir, val_dir, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "acfe1909",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.models.Sequential([\n",
    "    Flatten(input_shape = (256, 256, 3)),\n",
    "    Dense(1024, activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(512, activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(256, activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(10, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3acb0c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer = Adam(learning_rate = 0.001), loss = 'categorical_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2546013b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 196608)            0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1024)              201327616 \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 1024)              4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 201993482 (770.54 MB)\n",
      "Trainable params: 201989898 (770.53 MB)\n",
      "Non-trainable params: 3584 (14.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a4dc1ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "421/421 [==============================] - 130s 308ms/step - loss: 2.2450 - acc: 0.2062 - val_loss: 2.0980 - val_acc: 0.2486\n",
      "Epoch 2/15\n",
      "421/421 [==============================] - 129s 307ms/step - loss: 1.9323 - acc: 0.2980 - val_loss: 1.8307 - val_acc: 0.3330\n",
      "Epoch 3/15\n",
      "421/421 [==============================] - 130s 308ms/step - loss: 1.7795 - acc: 0.3521 - val_loss: 1.6930 - val_acc: 0.3879\n",
      "Epoch 4/15\n",
      "421/421 [==============================] - 129s 307ms/step - loss: 1.6590 - acc: 0.4014 - val_loss: 1.6717 - val_acc: 0.4008\n",
      "Epoch 5/15\n",
      "421/421 [==============================] - 130s 308ms/step - loss: 1.5434 - acc: 0.4395 - val_loss: 1.7376 - val_acc: 0.4028\n",
      "Epoch 6/15\n",
      "421/421 [==============================] - 131s 311ms/step - loss: 1.4629 - acc: 0.4733 - val_loss: 1.5790 - val_acc: 0.4347\n",
      "Epoch 7/15\n",
      "421/421 [==============================] - 131s 311ms/step - loss: 1.3844 - acc: 0.5023 - val_loss: 1.5370 - val_acc: 0.4753\n",
      "Epoch 8/15\n",
      "421/421 [==============================] - 129s 305ms/step - loss: 1.3396 - acc: 0.5211 - val_loss: 1.3931 - val_acc: 0.5030\n",
      "Epoch 9/15\n",
      "421/421 [==============================] - 130s 308ms/step - loss: 1.2612 - acc: 0.5445 - val_loss: 1.4833 - val_acc: 0.5088\n",
      "Epoch 10/15\n",
      "421/421 [==============================] - 130s 309ms/step - loss: 1.1897 - acc: 0.5707 - val_loss: 1.5497 - val_acc: 0.4805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x41846fb20>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(train_generator,\n",
    "                   epochs = 15,\n",
    "                   verbose = 1,\n",
    "                   validation_data = val_generator,\n",
    "                   callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e88667c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4492/4492 [==============================] - 186s 42ms/step - loss: 1.1901 - acc: 0.5828\n",
      "Accuracy based on simple Dense Model :- 58.28%\n"
     ]
    }
   ],
   "source": [
    "accuracy = model1.evaluate(test_generator)\n",
    "\n",
    "print(\"Accuracy based on simple Dense Model :- {:.2f}%\".format(accuracy[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510ab18d",
   "metadata": {},
   "source": [
    "## Second Model -> CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fde19556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13451 images belonging to 10 classes.\n",
      "Found 4481 images belonging to 10 classes.\n",
      "Found 4492 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator, val_generator, test_generator = imagedatageneration(train_dir, val_dir, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4fe4c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.models.Sequential([\n",
    "    Conv2D(32, (3, 3), activation = 'relu', input_shape = (256, 256, 3)),\n",
    "    Conv2D(32, (3, 3), activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation = 'relu'),\n",
    "    Conv2D(64, (3, 3), activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    Flatten(),\n",
    "    Dense(512, activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(256, activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(256, activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(128, activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(10, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "589515fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 254, 254, 32)      896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 252, 252, 32)      9248      \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 252, 252, 32)      128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 126, 126, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 124, 124, 64)      18496     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 122, 122, 64)      36928     \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 122, 122, 64)      256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 61, 61, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 59, 59, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 59, 59, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 29, 29, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 27, 27, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 27, 27, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 93312)             0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 512)               47776256  \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " batch_normalization_12 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_13 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48300586 (184.25 MB)\n",
      "Trainable params: 48297578 (184.24 MB)\n",
      "Non-trainable params: 3008 (11.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.compile(optimizer = Adam(learning_rate = 0.001), loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c4ff8ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "421/421 [==============================] - 491s 1s/step - loss: 1.6521 - acc: 0.4232 - val_loss: 2.0469 - val_acc: 0.3517\n",
      "Epoch 2/15\n",
      "421/421 [==============================] - 487s 1s/step - loss: 1.0345 - acc: 0.6482 - val_loss: 1.0111 - val_acc: 0.6474\n",
      "Epoch 3/15\n",
      "421/421 [==============================] - 475s 1s/step - loss: 0.7023 - acc: 0.7634 - val_loss: 1.5528 - val_acc: 0.5200\n",
      "Epoch 4/15\n",
      "421/421 [==============================] - 480s 1s/step - loss: 0.5491 - acc: 0.8157 - val_loss: 0.5341 - val_acc: 0.8199\n",
      "Epoch 5/15\n",
      "421/421 [==============================] - 478s 1s/step - loss: 0.4361 - acc: 0.8537 - val_loss: 0.5781 - val_acc: 0.8076\n",
      "Epoch 6/15\n",
      "421/421 [==============================] - 485s 1s/step - loss: 0.3842 - acc: 0.8712 - val_loss: 0.3223 - val_acc: 0.8880\n",
      "Epoch 7/15\n",
      "421/421 [==============================] - 482s 1s/step - loss: 0.3314 - acc: 0.8918 - val_loss: 0.5340 - val_acc: 0.8183\n",
      "Epoch 8/15\n",
      "421/421 [==============================] - 480s 1s/step - loss: 0.3009 - acc: 0.8974 - val_loss: 0.2815 - val_acc: 0.9087\n",
      "Epoch 9/15\n",
      "421/421 [==============================] - 479s 1s/step - loss: 0.2530 - acc: 0.9177 - val_loss: 0.3636 - val_acc: 0.8793\n",
      "Epoch 10/15\n",
      "421/421 [==============================] - 476s 1s/step - loss: 0.2468 - acc: 0.9184 - val_loss: 0.2569 - val_acc: 0.9154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x411067f70>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(train_generator,\n",
    "          epochs = 15,\n",
    "          verbose = 1,\n",
    "          validation_data = val_generator,\n",
    "          callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8e67bc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4492/4492 [==============================] - 134s 30ms/step - loss: 0.1879 - acc: 0.9437\n",
      "Accuracy based on our CNN Model :- 94.37%\n"
     ]
    }
   ],
   "source": [
    "accuracy = model2.evaluate(test_generator)\n",
    "\n",
    "print(\"Accuracy based on our CNN Model :- {:.2f}%\".format(accuracy[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d92d17",
   "metadata": {},
   "source": [
    "## Third Model -> VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5506cda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13451 images belonging to 10 classes.\n",
      "Found 4481 images belonging to 10 classes.\n",
      "Found 4492 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator, val_generator, test_generator = imagedatageneration(train_dir, val_dir, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "96b2c3fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 256, 256, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 256, 256, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 128, 128, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 128, 128, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 128, 128, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 64, 64, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 64, 64, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 64, 64, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 64, 64, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 32, 32, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 32, 32, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 16, 16, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 8, 8, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14714688 (56.13 MB)\n",
      "Trainable params: 14714688 (56.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = VGG16(weights = 'imagenet', include_top = False, input_shape = (256, 256, 3))\n",
    "pretrained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e9445f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in pretrained_model.layers[:-5]:\n",
    "    layer.trainable = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "23de928c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_layer = pretrained_model.get_layer('block5_pool')\n",
    "# last_output = last_layer.output\n",
    "\n",
    "model3 = tf.keras.models.Sequential([ \n",
    "    pretrained_model,\n",
    "    Flatten(),\n",
    "    Dense(512, activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(256, activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(128, activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(10, activation = 'softmax')\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9a1c638a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 8, 8, 512)         14714688  \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 32768)             0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 512)               16777728  \n",
      "                                                                 \n",
      " batch_normalization_20 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_21 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_22 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31661514 (120.78 MB)\n",
      "Trainable params: 24024458 (91.65 MB)\n",
      "Non-trainable params: 7637056 (29.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3.compile(optimizer = Adam(learning_rate = 0.001), loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bdfb59a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "421/421 [==============================] - 1341s 3s/step - loss: 0.9758 - acc: 0.6730 - val_loss: 1.6045 - val_acc: 0.6628\n",
      "Epoch 2/15\n",
      "421/421 [==============================] - 1319s 3s/step - loss: 0.3298 - acc: 0.8918 - val_loss: 0.7769 - val_acc: 0.7563\n",
      "Epoch 3/15\n",
      "421/421 [==============================] - 1318s 3s/step - loss: 0.1872 - acc: 0.9409 - val_loss: 0.4495 - val_acc: 0.8525\n",
      "Epoch 4/15\n",
      "421/421 [==============================] - 1318s 3s/step - loss: 0.1743 - acc: 0.9457 - val_loss: 0.2425 - val_acc: 0.9232\n",
      "Epoch 5/15\n",
      "421/421 [==============================] - 3260s 8s/step - loss: 0.1477 - acc: 0.9525 - val_loss: 1.5301 - val_acc: 0.6217\n",
      "Epoch 6/15\n",
      "421/421 [==============================] - 1823s 4s/step - loss: 0.1285 - acc: 0.9616 - val_loss: 0.8088 - val_acc: 0.7717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x3574a5330>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(train_generator,\n",
    "          epochs = 15,\n",
    "          verbose = 1,\n",
    "          validation_data = val_generator,\n",
    "          callbacks = [es])\n",
    "\n",
    "\n",
    "# model3.fit(train_generator,\n",
    "#           steps_per_epoch = 250,\n",
    "#           epochs = 20,\n",
    "#           verbose = 1,\n",
    "#           validation_steps = 50,\n",
    "#           validation_data = val_generator,\n",
    "#           callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3ebedf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4492/4492 [==============================] - 381s 85ms/step - loss: 0.7634 - acc: 0.7892\n",
      "Accuracy based on our VGG16 Model :- 78.92%\n"
     ]
    }
   ],
   "source": [
    "accuracy = model3.evaluate(test_generator)\n",
    "\n",
    "print(\"Accuracy based on our VGG16 Model :- {:.2f}%\".format(accuracy[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cda422",
   "metadata": {},
   "source": [
    "## Fourth Model -> ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "12197747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13451 images belonging to 10 classes.\n",
      "Found 4481 images belonging to 10 classes.\n",
      "Found 4492 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator, val_generator, test_generator = imagedatageneration(train_dir, val_dir, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e84f158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = ResNet50(weights = 'imagenet', include_top = False, input_shape = (256, 256, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8ea23432",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in pretrained_model.layers[:-3]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ea82bf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = tf.keras.models.Sequential([ \n",
    "    pretrained_model,\n",
    "    Flatten(),\n",
    "    Dense(512, activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(256, activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(128, activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(10, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0d04691b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 131072)            0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 512)               67109376  \n",
      "                                                                 \n",
      " batch_normalization_17 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_18 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_19 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90866186 (346.63 MB)\n",
      "Trainable params: 67280778 (256.66 MB)\n",
      "Non-trainable params: 23585408 (89.97 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4.compile(optimizer = Adam(learning_rate = 0.001), loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3b71c931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "421/421 [==============================] - 430s 1s/step - loss: 1.7425 - acc: 0.3945 - val_loss: 3.1864 - val_acc: 0.2593\n",
      "Epoch 2/15\n",
      "421/421 [==============================] - 430s 1s/step - loss: 1.1782 - acc: 0.5933 - val_loss: 1.1323 - val_acc: 0.6025\n",
      "Epoch 3/15\n",
      "421/421 [==============================] - 464s 1s/step - loss: 0.9540 - acc: 0.6779 - val_loss: 0.9264 - val_acc: 0.6791\n",
      "Epoch 4/15\n",
      "421/421 [==============================] - 642s 2s/step - loss: 0.8344 - acc: 0.7136 - val_loss: 0.9784 - val_acc: 0.6809\n",
      "Epoch 5/15\n",
      "421/421 [==============================] - 970s 2s/step - loss: 0.7440 - acc: 0.7494 - val_loss: 0.7869 - val_acc: 0.7228\n",
      "Epoch 6/15\n",
      "421/421 [==============================] - 412s 978ms/step - loss: 0.6847 - acc: 0.7671 - val_loss: 0.9296 - val_acc: 0.6967\n",
      "Epoch 7/15\n",
      "421/421 [==============================] - 416s 989ms/step - loss: 0.6348 - acc: 0.7869 - val_loss: 0.7000 - val_acc: 0.7581\n",
      "Epoch 8/15\n",
      "421/421 [==============================] - 494s 1s/step - loss: 0.6021 - acc: 0.7996 - val_loss: 0.7811 - val_acc: 0.7425\n",
      "Epoch 9/15\n",
      "421/421 [==============================] - 413s 982ms/step - loss: 0.5568 - acc: 0.8136 - val_loss: 0.5598 - val_acc: 0.8150\n",
      "Epoch 10/15\n",
      "421/421 [==============================] - 417s 992ms/step - loss: 0.5315 - acc: 0.8193 - val_loss: 0.5627 - val_acc: 0.8079\n",
      "Epoch 11/15\n",
      "421/421 [==============================] - 433s 1s/step - loss: 0.5000 - acc: 0.8332 - val_loss: 0.6602 - val_acc: 0.7817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x355ff2890>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.fit(train_generator,\n",
    "          epochs = 15,\n",
    "          verbose = 1,\n",
    "          validation_data = val_generator,\n",
    "          callbacks = [es])\n",
    "\n",
    "\n",
    "# model4.fit(train_generator,\n",
    "#           steps_per_epoch = 250,\n",
    "#           epochs = 20,\n",
    "#           verbose = 1,\n",
    "#           validation_steps = 50,\n",
    "#           validation_data = val_generator,\n",
    "#           callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4aff72c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4492/4492 [==============================] - 352s 78ms/step - loss: 0.5178 - acc: 0.8257\n",
      "Accuracy based on our ResNet50 Model :- 82.57%\n"
     ]
    }
   ],
   "source": [
    "accuracy = model4.evaluate(test_generator)\n",
    "\n",
    "print(\"Accuracy based on our ResNet50 Model :- {:.2f}%\".format(accuracy[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eeff80",
   "metadata": {},
   "source": [
    "## Fifth Model -> Yolo v8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083ce148",
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = YOLO('yolov8n-cls.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3465de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitfolders.ratio(\"../datasets/state-farm-distracted-driver-detection/imgs/train\", output=\"../datasets/state-farm-distracted-driver-detection/output\", seed = 1337, ratio=(0.7, 0.15, 0.15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267a0be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model5.train(data = \"../datasets/state-farm-distracted-driver-detection/output\", epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2452028",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db35a99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0dbaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('./runs/classify/train2/results.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e82abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"./runs/classify/train2/results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b0d0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"./runs/classify/train2/confusion_matrix_normalized.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d3b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']\n",
    "# path = \"../datasets/state-farm-distracted-driver-detection/output/test/c1/\"\n",
    "# actual_class = 1\n",
    "# model_weights = \"./runs/classify/train2/weights/best.pt\"\n",
    "# pred = [(path+i,model5.predict(path+i, model = model_weights)[0].probs.top1, actual_class) for i in os.listdir(path)[:45]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a27dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']\n",
    "test_images_path = \"../datasets/state-farm-distracted-driver-detection/output/test/\"\n",
    "model_weights = \"./runs/classify/train2/weights/best.pt\"\n",
    "\n",
    "predicted_list = []\n",
    "\n",
    "for clas in classes:\n",
    "    image_dir = os.path.join(test_images_path, clas)\n",
    "    # print(image_dir)\n",
    "    images_list = os.listdir(image_dir)\n",
    "    # print(images_list)\n",
    "    # Class label in the form of 0 to 9\n",
    "    class_label = int(clas[-1])\n",
    "    # print(class_label)\n",
    "    for image in images_list:\n",
    "        path = os.path.join(image_dir, image)\n",
    "        # print(path)\n",
    "        y_actual = class_label\n",
    "        y_predicted = model5.predict(path, model = model_weights)[0].probs.top1\n",
    "        predicted_list.append([path, y_actual, y_predicted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2c4332",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"length of the Predicted List : \", len(predicted_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017c6a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(predicted_list, columns = ['Image_path', 'Y_actual', 'Y_predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a39bbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(df['Y_actual'], df['Y_predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2ba1a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
